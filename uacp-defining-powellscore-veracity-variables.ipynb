{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/uacp-defining-powellscore-veracity-variables?scriptVersionId=143351799\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 2.0 UACP - Defining PowellScore & Veracity Variables\n## [1.0 UAP Analytic Centralization Program](https://www.kaggle.com/code/dascient/uacp-uap-analytic-centralization-program)\n## [NLP - Sentiment Intensity Analyzer](https://github.com/cjhutto/vaderSentiment) Against Reporting Comments\n\n### In collaboration with The Scientific Coalition for UAP Studies [(SCU)](ExploreSCU.org).\n​\nHere we isolate only pertinent variables from the original dataset. We've also decided to leave open most of the code cells below; enabling transparency on foundation of all variables. ","metadata":{}},{"cell_type":"code","source":"# for the sake of expeditious analysis\n!pip install xlrd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom shapely.geometry import Point\nimport geopandas as gpd\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom geopandas import GeoDataFrame\nimport matplotlib.colors as colors\nimport seaborn as sns\nimport random as r\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        #print('Files loaded.')\n        \npd.set_option('display.max_colwidth', None)\n\n# loading first nuforc dataframe\nog_df1 = pd.read_csv('/kaggle/input/ufo-sightings/ufos.csv',header=0)\ndf = og_df1.dropna().copy()\nog_df2 = pd.read_csv('/kaggle/input/d/NUFORC/ufo-sightings/scrubbed.csv',header=0)\ndf2 = og_df2.dropna().copy()\n\n#############################################\n\nlex = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/UFO lexicon rev2.xls',sheet_name='Sheet1',header=7)\nlex = lex.dropna(how='all').drop(columns='Unnamed: 0').copy()\n\n#############################################\n# sanitize\n# drop some columns, for now\ndf = df.drop(columns=['datetime','duration (hours/min)'])\n\n# date posted deemed to be easily conveible to timestamp values, so i'm gonna work with that for now.\ndf['date posted'] = df['date posted'].astype('datetime64[ns]')\n\n\n# length of comments\ndf['comment_length'] = [len(str(v[0:500])) for i,v in df.comments.items()]\n\n\n# convert seconds to minutes\ndf[\"duration (minutes)\"] = [int(v)/60 for i,v in df[\"duration (seconds)\"].items()]\n\n\n# creating Geo Point column for sopecial use below\ndf['Geo Point'] = df.apply(lambda x:'%s, %s' % (x['latitude'],x['longitude']),axis=1)\n\n\n# let's create subsets of our 80,000 here: \n# we can implement conditionals, remove/analyze outliers, \n# & will enable for back referencing when starting to run \n# robust AI-ML modeling that would otherwise take much longer to run.\n\n# let's create subsets from the main dataframe/reporting-data w/ respect to duration of observations\ndf_under100 = df[df[\"duration (minutes)\"]<100]\ndf_under60 = df[df[\"duration (minutes)\"]<60]\n\n# random binary column for future AI-ML modeling.\na=['balloon','spacejunk','sensor_malfunction','undentified','anomalous']     \ndf['verified'] = pd.Series(r.choices(a,k=len(df),weights=(50, 40, 30, 20, 10)),index=df.index)\n\n# shape-focused\ncircles = df[df['shape'] == 'circle']\nspheres = df[df['shape'] == 'sphere']\nlights = df[df['shape'] == 'light']\nteardrops = df[df['shape'] == 'teardrop']\n\n# year-month\ndf['year_month'] = df['date posted'].dt.to_period('M')\n\nclear_output()\n# show\nprint(\"\\nOriginal dataset.\")\nprint(f\"\\nReports: {len(df)} non-null dataframe.\")\nprint(\"\\nMatrix:\",df.shape[0],\"rows,\",df.shape[1],\"columns\")\ndf = df.sort_values('date posted',ascending=True).reset_index(drop=True)\ndf.tail(11).reset_index(drop=True).style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'}).set_properties(**{'text-align': 'left'})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-17T22:32:12.099757Z","iopub.execute_input":"2023-09-17T22:32:12.100191Z","iopub.status.idle":"2023-09-17T22:32:30.329037Z","shell.execute_reply.started":"2023-09-17T22:32:12.100159Z","shell.execute_reply":"2023-09-17T22:32:30.327946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lexicon","metadata":{}},{"cell_type":"code","source":"#lex[lex['RATING']!=0]\n#lex[lex['Previous Rating']!=0]\n# non-zero rating words\nlex_nonzero = lex[lex['RATING']!=0]\n\n#lex[lex['Previous Rating']>=3]\n# rating words gerater than or equal to 3\n#lex_nonzero = lex[lex['RATING']>=3]\nlex_nonzero","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T00:06:11.728391Z","iopub.execute_input":"2023-09-18T00:06:11.728792Z","iopub.status.idle":"2023-09-18T00:06:11.748033Z","shell.execute_reply.started":"2023-09-18T00:06:11.728761Z","shell.execute_reply":"2023-09-18T00:06:11.746398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hash through each comment to find only those that include non-zero lexicon words. ","metadata":{}},{"cell_type":"code","source":"%%time\nimport time\nfrom nltk.tokenize import word_tokenize\n\ndf_sample = df #.sample(500)\n\n# hash through each comment to find only those that include non-zero lexicon words\nlexicon_favored = df_sample.copy()\nlexicon_favored['rating'] = pd.Series()\nlexicon_favored['lexicon_word'] = pd.Series()\n\nfor i,word in lex_nonzero.WORD.items():\n    for i2,piece in df_sample.comments.items():\n        if word in word_tokenize(piece.lower()):\n            #print('index',i2,'\\nword',word, '\\npiece',piece.lower(), '\\nrating', lex_nonzero.RATING[i],'\\n')\n\n            # add rating from lexicon\n            lexicon_favored['rating'][i2] = lex_nonzero.RATING[i]\n\n            # add up every word usage in comments\n            lexicon_favored['lexicon_word'][i2] = lex_nonzero.WORD[i]\n\nlexicon_favored = lexicon_favored\n#clear_output()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-18T00:15:13.79255Z","iopub.execute_input":"2023-09-18T00:15:13.792963Z","iopub.status.idle":"2023-09-18T01:52:10.730558Z","shell.execute_reply.started":"2023-09-18T00:15:13.792932Z","shell.execute_reply":"2023-09-18T01:52:10.72957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VADER","metadata":{}},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n# https://github.com/cjhutto/vaderSentiment\n# import SentimentIntensityAnalyzer class\n# from vaderSentiment.vaderSentiment module.\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# function to print sentiments\n# of the sentence.\ndef sentiment_scores(sentence):\n\n    # Create a SentimentIntensityAnalyzer object.\n    sid_obj = SentimentIntensityAnalyzer()\n    sentiment_dict = sid_obj.polarity_scores(sentence)\n    \n    # create a list\n    results = []\n    results.append({\"% Positive\":sentiment_dict['pos'],\n                    \"% Negative\":sentiment_dict['neg'],\n                    \"% Neutral\":sentiment_dict['neu']\n                   })\n    results = pd.DataFrame(results)\n    return results\n\n# Apply to df['comments'] column.\ndef NLP_PowellScore(commentsColumns):\n    \n    # obtain each comment for 'comments' column\n    eachComment = [eachComment for i,eachComment in commentsColumns.items()]\n    eachComment = pd.Series(eachComment)\n                               \n    # vader.variables.PowellScore\n    PowellPositive = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Positive\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellNegative = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Negative\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellNeutral = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Neutral\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    \n    return PowellPositive,PowellNegative,PowellNeutral","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:10.732803Z","iopub.execute_input":"2023-09-18T01:52:10.733165Z","iopub.status.idle":"2023-09-18T01:52:10.744088Z","shell.execute_reply.started":"2023-09-18T01:52:10.733131Z","shell.execute_reply":"2023-09-18T01:52:10.743175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample of 500 reports sorted by Veracity\nWe also added the \"Rating\" score from Lexicon.","metadata":{}},{"cell_type":"code","source":"%%time\n# defining Powell Scores by sentiment outputs: Positive, Negative, Neutral, & Rating\n\n# let's only take a small sample - this will definitely take a few minutes, grab yourself some water...\nrobert = lexicon_favored.sample(500).copy()\n\nrobert[\"PowellPositive\"] = NLP_PowellScore(robert['comments'])[0]\nrobert[\"PowellNegative\"] = NLP_PowellScore(robert['comments'])[1]\nrobert[\"PowellNeutral\"] = NLP_PowellScore(robert['comments'])[2]\n\n# PowellScore \nrobert[\"PowellScore\"] = (robert[\"PowellPositive\"]-robert[\"PowellNegative\"])/robert[\"PowellNeutral\"]\n\n# veracity\nrobert[\"veracity\"] = robert[\"PowellScore\"]*robert[\"comment_length\"]*robert[\"rating\"] # FINALLY, THIS EQUATION ACCOUNTS FOR POWELL'S LEXICON RATINGS!\n\n# veracity is still very much in progress. we are looking for ways forward to \n# better define them. although, it is important to note that \"veracity\" will \n# be variable that is subjective to the type of datasets.\ncolumns = ['date posted','city','state','shape','comments','comment_length',\\\n        'latitude','longitude','duration (minutes)','PowellPositive',\\\n        'PowellNegative','PowellNeutral','PowellScore','veracity','rating','lexicon_word']\n\ndf1 = robert[columns].sort_values('veracity',ascending=False).reset_index(drop=True)\ndf1[df1['comment_length']>10].head(20)\\\n        .style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:10.74531Z","iopub.execute_input":"2023-09-18T01:52:10.745734Z","iopub.status.idle":"2023-09-18T01:52:52.31144Z","shell.execute_reply.started":"2023-09-18T01:52:10.745702Z","shell.execute_reply":"2023-09-18T01:52:52.310422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ovals seen within 162 Miles of Imperial Beach & Blythe, California","metadata":{}},{"cell_type":"code","source":"%%time\n# ca_oval\nca_oval = df1[df1.state=='ca'].reset_index(drop=True)\nca_oval = ca_oval[ca_oval['shape']=='oval']\n\n# only ovals\nca_oval_162 = ca_oval.sort_values(['latitude','longitude'])\nrobert_ca_oval_162 = ca_oval_162\nrobert_ca_oval_162[\"PowellPositive\"] = NLP_PowellScore(robert_ca_oval_162['comments'])[0]\nrobert_ca_oval_162[\"PowellNegative\"] = NLP_PowellScore(robert_ca_oval_162['comments'])[1]\nrobert_ca_oval_162[\"PowellNeutral\"] = NLP_PowellScore(robert_ca_oval_162['comments'])[2]\n\n# PowellScore \nrobert_ca_oval_162[\"PowellScore\"] = (robert_ca_oval_162[\"PowellPositive\"]-robert_ca_oval_162[\"PowellNegative\"])/robert_ca_oval_162[\"PowellNeutral\"]\n\n# veracity\nrobert_ca_oval_162[\"veracity\"] = robert_ca_oval_162[\"PowellScore\"]*robert_ca_oval_162[\"comment_length\"]*robert_ca_oval_162[\"rating\"]\n\n\ndf2 = robert_ca_oval_162[['date posted','city','state','shape','comments','comment_length','duration (minutes)',\\\n        'latitude','longitude','PowellPositive','PowellScore','veracity']].sort_values(['veracity'],ascending=False).reset_index(drop=True)\ndf2.head(50).style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:52.314136Z","iopub.execute_input":"2023-09-18T01:52:52.314756Z","iopub.status.idle":"2023-09-18T01:52:52.654709Z","shell.execute_reply.started":"2023-09-18T01:52:52.314721Z","shell.execute_reply":"2023-09-18T01:52:52.653726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Powell Variables in 3D\nThis is a 3D-interactive chart that uses the date posted, veracity, & PowellScore variables. Colored by lexicon rating. Sized by comment_length.\n\nBy definition, these actually render 5-Dimensional charts, if one considers veracity & commenth lengths of reports as 'features of a situation'.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\ndf_plot = df1[df1.rating>0]\n# graph\nfig = px.scatter_3d(df_plot, x='date posted', y='veracity', z='PowellScore',\n              color='rating',\n              size = 'comment_length',\n              hover_name = 'city',\n              hover_data=['city','state','comments','rating','lexicon_word'],              \n              opacity=0.5,\n              size_max=17\n                   )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:52.656179Z","iopub.execute_input":"2023-09-18T01:52:52.657023Z","iopub.status.idle":"2023-09-18T01:52:52.762802Z","shell.execute_reply.started":"2023-09-18T01:52:52.656989Z","shell.execute_reply":"2023-09-18T01:52:52.761792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This one shows Date Posted vs PowellScore & Lexicon Rating Variables of California Oval reports. Colored by Veracity.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# graph\nfig = px.scatter_3d(df1, x='date posted', y='PowellScore', z='rating',\n              color='veracity',\n              size = 'comment_length',\n              hover_name = 'city',\n              hover_data=['city','state','comments','rating','lexicon_word'],              \n              opacity=0.5,\n              size_max=17\n                   )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:52.764011Z","iopub.execute_input":"2023-09-18T01:52:52.764764Z","iopub.status.idle":"2023-09-18T01:52:52.878439Z","shell.execute_reply.started":"2023-09-18T01:52:52.764732Z","shell.execute_reply":"2023-09-18T01:52:52.87763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ovals seen between Imperial Beach & Blythe, California - Date Posted vs PowellNeutral vs PowellScore","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# graph\nfig = px.scatter_3d(robert_ca_oval_162, x='date posted', y='PowellNeutral', z='rating',\n              color='veracity',\n              size = 'comment_length',\n              hover_name = 'city',\n              hover_data=['city','state','comments','shape','veracity','rating'],              \n              opacity=0.5,\n              size_max=17\n                   )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:52.879882Z","iopub.execute_input":"2023-09-18T01:52:52.880554Z","iopub.status.idle":"2023-09-18T01:52:52.961458Z","shell.execute_reply.started":"2023-09-18T01:52:52.880519Z","shell.execute_reply":"2023-09-18T01:52:52.96054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These variables are still very much in progress & there currently is no process for defining them. Despite the disparate, disconnected, & wide range of skeptic/non-skeptic relational databases — we have managed to connect with organizations that promote open source — public repositories & most are willing to coordinate with one another in developing a UAP Reporting & Events Hub. Wherein all pertinent reports, sightings, measurements, & signatures are to be populated by various factors from multiple disciplines & technologies. We will do our best to coordinate with prominent key members of the UAP community in order to contribute to building out a “standardized” reporting mechanism in an intelligible & non-duplicative fashion. We are looking for ways forward in getting access to real-time, current reports.\n\nThe goal would be to create something similar to an Order of Battle, so that reports at specific times & locations can be compared to past reports to augment credibility determination, as well as eventually be compared to known events that may explain them. Once those explanations are vetted, reports would be coded by likelihood of mundane vs anomalous, which would aid in the processing of similar events in the future.\n\nIn addition, we have already begun looking for trends over time, such as the time of day when reported events take place, & the type of object reported over the decades. The latter can be observed in the “Shapes by Share of Reports” chart, which provides indications of confirmation bias in observed behavior.\n\nFinally, big data analysis (alongside robust AI|ML|DS modeling techniques) could also provide insight into the development of improved collection & reporting processes, which currently appear to be undefined, improving the quality of the data we receive. — K. Kolbe.","metadata":{}},{"cell_type":"markdown","source":"# Different NLP Methods","metadata":{}},{"cell_type":"markdown","source":"# DaS-VADER Sentiment Analyzer\n\nHere, we begin focusing on the lexicon analysis of each comment submission.","metadata":{}},{"cell_type":"code","source":"df2[['comments']].sample(15).style.set_properties(**{'text-align': 'left'})","metadata":{"execution":{"iopub.status.busy":"2023-09-18T02:09:31.321496Z","iopub.execute_input":"2023-09-18T02:09:31.321896Z","iopub.status.idle":"2023-09-18T02:09:31.334663Z","shell.execute_reply.started":"2023-09-18T02:09:31.321864Z","shell.execute_reply":"2023-09-18T02:09:31.333718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets split up every word from every column & frame that in itself, call it \"words\"\na = [v.split(' ') for i,v in df1.comments.items()]\nflatlist=[]\nfor sublist in a:\n    for element in sublist:\n        flatlist.append(element)\ncomments = pd.DataFrame(flatlist, columns=['words'])\ncomments","metadata":{"execution":{"iopub.status.busy":"2023-09-18T02:09:52.786526Z","iopub.execute_input":"2023-09-18T02:09:52.787428Z","iopub.status.idle":"2023-09-18T02:09:52.822077Z","shell.execute_reply.started":"2023-09-18T02:09:52.787383Z","shell.execute_reply":"2023-09-18T02:09:52.820966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\n\ncomments.words.value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-18T02:10:00.169687Z","iopub.execute_input":"2023-09-18T02:10:00.170059Z","iopub.status.idle":"2023-09-18T02:10:00.192119Z","shell.execute_reply.started":"2023-09-18T02:10:00.170031Z","shell.execute_reply":"2023-09-18T02:10:00.190846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments.words.value_counts().head(50).plot(kind='barh',figsize=(15,7))","metadata":{"execution":{"iopub.status.busy":"2023-09-18T02:10:02.938592Z","iopub.execute_input":"2023-09-18T02:10:02.939545Z","iopub.status.idle":"2023-09-18T02:10:03.663729Z","shell.execute_reply.started":"2023-09-18T02:10:02.939497Z","shell.execute_reply":"2023-09-18T02:10:03.662647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# billy-boy!\n# isolate only \"adjectives, nouns, verbs, & adverbs\"","metadata":{"execution":{"iopub.status.busy":"2023-09-18T01:52:53.046159Z","iopub.status.idle":"2023-09-18T01:52:53.046925Z","shell.execute_reply.started":"2023-09-18T01:52:53.046687Z","shell.execute_reply":"2023-09-18T01:52:53.04671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visual Insights","metadata":{}},{"cell_type":"markdown","source":"## Word Clouds\n### All 500 samples.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# in the clouds\n# 500 samples \ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df1.comments:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 7,\n                colormap='twilight').generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize=(25,10), facecolor='None')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T02:10:17.779608Z","iopub.execute_input":"2023-09-18T02:10:17.780584Z","iopub.status.idle":"2023-09-18T02:10:19.621942Z","shell.execute_reply.started":"2023-09-18T02:10:17.780547Z","shell.execute_reply":"2023-09-18T02:10:19.618181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### California Ovals ","metadata":{}},{"cell_type":"code","source":"# in the clouds\n# california ovals\ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df2.comments:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 7,\n                colormap='twilight').generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize=(25,10), facecolor='None')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-18T02:10:19.774398Z","iopub.execute_input":"2023-09-18T02:10:19.774857Z","iopub.status.idle":"2023-09-18T02:10:20.831419Z","shell.execute_reply.started":"2023-09-18T02:10:19.774827Z","shell.execute_reply":"2023-09-18T02:10:20.830615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Veracity Comments","metadata":{}},{"cell_type":"code","source":"df1.reset_index(drop=True).set_index('veracity').head(20)[['comments','rating','shape','city']]","metadata":{"execution":{"iopub.status.busy":"2023-09-18T02:11:17.434891Z","iopub.execute_input":"2023-09-18T02:11:17.43526Z","iopub.status.idle":"2023-09-18T02:11:17.460094Z","shell.execute_reply.started":"2023-09-18T02:11:17.43523Z","shell.execute_reply":"2023-09-18T02:11:17.459021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### California Ovals Comments","metadata":{}},{"cell_type":"code","source":"df2.set_index('veracity').comments","metadata":{"execution":{"iopub.status.busy":"2023-09-18T02:10:24.816234Z","iopub.execute_input":"2023-09-18T02:10:24.8166Z","iopub.status.idle":"2023-09-18T02:10:24.826395Z","shell.execute_reply.started":"2023-09-18T02:10:24.816572Z","shell.execute_reply":"2023-09-18T02:10:24.825155Z"},"trusted":true},"execution_count":null,"outputs":[]}]}