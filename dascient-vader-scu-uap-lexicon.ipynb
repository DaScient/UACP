{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/dascient-vader-scu-uap-lexicon?scriptVersionId=144273619\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Natural Language Toolkit: vader - TAMPERED w/ SCU Lexicon\n#\n# Copyright (C) 2001-2023 NLTK Project\n# Author: C.J. Hutto <Clayton.Hutto@gtri.gatech.edu>\n#         Ewan Klein <ewan@inf.ed.ac.uk> (modifications)\n#         Pierpaolo Pantone <24alsecondo@gmail.com> (modifications)\n#         George Berry <geb97@cornell.edu> (modifications)\n#         Malavika Suresh <malavika.suresh0794@gmail.com> (modifications)\n# URL: <https://www.nltk.org/>\n# For license information, see LICENSE.TXT\n#\n# Modifications to the original VADER code have been made in order to\n# integrate it into NLTK. These have involved changes to\n# ensure Python 3 compatibility, and refactoring to achieve greater modularity.\n\n\"\"\"\nIf you use the VADER sentiment analysis tools, please cite:\n\nHutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for\nSentiment Analysis of Social Media Text. Eighth International Conference on\nWeblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n\"\"\"\n\nimport math\nimport re\nimport string\nfrom itertools import product\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output\nimport nltk.data\nclear_output()\n\nclass VaderConstants:\n    \"\"\"\n    A class to keep the Vader lists and constants.\n    \"\"\"\n\n    ##Constants##\n    # (empirically derived mean sentiment intensity rating increase for booster words)\n    B_INCR = 0.293\n    B_DECR = -0.293\n\n    # (empirically derived mean sentiment intensity rating increase for using\n    # ALLCAPs to emphasize a word)\n    C_INCR = 0.733\n\n    N_SCALAR = -0.74\n\n    NEGATE = {'UFO',\n             'again',\n             'alien',\n             'another',\n             'attack',\n             'beam',\n             'beautiful',\n             'bed',\n             'bedroom',\n             'being',\n             'believe',\n             'capture',\n             'caught',\n             'chase',\n             'chemtrail',\n             'cloud',\n             'clouds',\n             'coming',\n             'confuse',\n             'contact',\n             'daily',\n             \"didn't\",\n             'discover',\n             'dream',\n             'earth',\n             'every',\n             'excited',\n             'experience',\n             'figure',\n             'fireball',\n             'firework',\n             'fireworks',\n             'frequent',\n             'happy',\n             'help',\n             'hope',\n             'less',\n             'me',\n             'mufon',\n             'needle',\n             'nightly',\n             'often',\n             'orange',\n             'pain',\n             'paranormal',\n             'please',\n             'presence',\n             'psychic',\n             'review',\n             'room',\n             'scared',\n             'seen',\n             'shock',\n             'shocked',\n             'skin',\n             'sleep',\n             'star-like',\n             'starlike',\n             'starship',\n             'storm',\n             'surgery',\n             'taken',\n             'their',\n             'thigh',\n             'tragedy',\n             'trance',\n             'ufo',\n             \"ufo's\",\n             'ufos',\n             'visit',\n             'visitation',\n             'want',\n             'zoom',\n             'zoomed'\n             }\n\n\n    # booster/dampener 'intensifiers' or 'degree adverbs'\n    # https://en.wiktionary.org/wiki/Category:English_degree_adverbs\n\n    BOOSTER_DICT = {\n        'northeastern'  : B_INCR,\n        'law'  : B_INCR,\n        'duty'  : B_INCR,\n        'appearing'  : B_INCR,\n        'background'  : B_INCR,\n        'clockwise'  : B_INCR,\n        'momentarily'  : B_INCR,\n        'degrees'  : B_INCR,\n        'appearance'  : B_INCR,\n        'approx'  : B_INCR,\n        'approximate'  : B_INCR,\n        'within'  : B_INCR,\n        'NORAD'  : B_INCR,\n        'plasma'  : B_INCR,\n        'astronomical'  : B_INCR,\n        'Lockheed'  : B_INCR,\n        'retire'  : B_INCR,\n        'locomotion'  : B_INCR,\n        'feather'  : B_INCR,\n        'seemed'  : B_INCR,\n        'witnesses'  : B_INCR,\n        'aviation'  : B_INCR,\n        'nuclear'  : B_INCR,\n        'air force'  : B_INCR,\n        'north'  : B_INCR,\n        'seem'  : B_INCR,\n        'fort'  : B_INCR,\n        'register'  : B_INCR,\n        'translucent'  : B_INCR,\n        'hexagon'  : B_INCR,\n        'normal'  : B_INCR,\n        'target'  : B_INCR,\n        'morning'  : B_INCR,\n        'rotar'  : B_INCR,\n        'southwest'  : B_INCR,\n        'AF'  : B_INCR,\n        'south'  : B_INCR,\n        'approach'  : B_INCR,\n        'opaque'  : B_INCR,\n        'hexagonal'  : B_INCR,\n        'guard'  : B_INCR,\n        'briefly'  : B_INCR,\n        'rectangular'  : B_INCR,\n        'upward'  : B_INCR,\n        'officer'  : B_INCR,\n        'telescope'  : B_INCR,\n        'propulsion'  : B_INCR,\n        'perimeter'  : B_INCR,\n        'traverse'  : B_INCR,\n        'army'  : B_INCR,\n        'eastern'  : B_INCR,\n        'elongate'  : B_INCR,\n        'customer'  : B_INCR,\n        'binoculars'  : B_INCR,\n        'pulled'  : B_INCR,\n        'emanate'  : B_INCR,\n        'scope'  : B_INCR,\n        'southern'  : B_INCR,\n        'assume'  : B_INCR,\n        'capability'  : B_INCR,\n        'radar'  : B_INCR,\n        'structure'  : B_INCR,\n        'tree-line'  : B_INCR,\n        'appear'  : B_INCR,\n        'western'  : B_INCR,\n        'defense'  : B_INCR,\n        'irregular'  : B_INCR,\n        'call'  : B_INCR,\n        'analyze'  : B_INCR,\n        'coast  '  : B_INCR,\n        'vertex'  : B_INCR,\n        'altitude'  : B_INCR,\n        'grid'  : B_INCR,\n        'blend'  : B_INCR,\n        'public'  : B_INCR,\n        'northern'  : B_INCR,\n        'simultaneous'  : B_INCR,\n        'perspective'  : B_INCR,\n        'capable'  : B_INCR,\n        'navy'  : B_INCR,\n        'degree'  : B_INCR,\n        'family'  : B_INCR,\n        'east'  : B_INCR,\n        'enforcement'  : B_INCR,\n        'position'  : B_INCR,\n        'F14'  : B_INCR,\n        'astronomy'  : B_INCR,\n        'police'  : B_INCR,\n        'base'  : B_INCR,\n        'security'  : B_INCR,\n        'velocity'  : B_INCR,\n        'similar'  : B_INCR,\n        'civilian'  : B_INCR,\n        'pilot'  : B_INCR,\n        'F15'  : B_INCR,\n        'southeastern'  : B_INCR,\n        'Boeing'  : B_INCR,\n        'counterclockwise'  : B_INCR,\n        'estimate'  : B_INCR,\n        'sentry'  : B_INCR,\n        'sheriff'  : B_INCR,\n        'assign'  : B_INCR,\n        'B2'  : B_INCR,\n        'cloak'  : B_INCR,\n        'west'  : B_INCR,\n        'northwest'  : B_INCR,\n        'military'  : B_INCR,\n        'marine'  : B_INCR,\n        'engine'  : B_INCR,\n        'reception'  : B_INCR,\n        'treeline'  : B_INCR,\n        '911'  : B_INCR,\n        'radio'  : B_INCR,\n        'southeast'  : B_INCR,\n        'height'  : B_INCR,\n        'rotating'  : B_INCR,\n        'rotate'  : B_INCR,\n        'octagonal'  : B_INCR,\n        'day'  : B_INCR,\n        'physics'  : B_INCR,\n        'border'  : B_INCR,\n        'northwestern'  : B_INCR,\n        'cluster'  : B_INCR,\n        'appeared'  : B_INCR,\n        'solid'  : B_INCR,\n        'rotary'  : B_INCR,\n        'horizontal'  : B_INCR,\n        'approximately'  : B_INCR,\n        'equidistant'  : B_INCR,\n        'independent'  : B_INCR,\n        'naval'  : B_INCR,\n        'policeman'  : B_INCR,\n        'resemble'  : B_INCR,\n        'northeast'  : B_INCR,\n        'commercial'  : B_INCR,\n        'southwestern'  : B_INCR,\n        'octagon'  : B_INCR,\n        'motion'  : B_INCR,\n        'deputy'  : B_INCR,\n        'daytime'  : B_INCR,\n        'flight'  : B_INCR,\n        'biologist'  : B_INCR,\n        'silo'  : B_INCR,\n        'patrol'  : B_INCR,\n        'chemist'  : B_INCR,\n        'perfect'  : B_INCR,\n        'astronomer'  : B_INCR,\n        'azimuth'  : B_INCR,\n        'radioed'  : B_INCR,\n        'elevation'  : B_INCR,\n        'coast guard'  : B_INCR,\n        'engineer'  : B_INCR,\n        'reports'  : B_DECR,\n        'towards'  : B_DECR,\n        'directions'  : B_DECR,\n        'attention'  : B_DECR,\n        'directly'  : B_DECR,\n        'friends'  : B_DECR,\n        'triangle'  : B_DECR,\n        'ft'  : B_DECR,\n        'pictures'  : B_DECR,\n        'event'  : B_DECR,\n        'father'  : B_DECR,\n        'observe '  : B_DECR,\n        'observing'  : B_DECR,\n        'extremely'  : B_DECR,\n        'standing'  : B_DECR,\n        'visible'  : B_DECR,\n        'spherical'  : B_DECR,\n        'angle'  : B_DECR,\n        'hover'  : B_DECR,\n        'triangular'  : B_DECR,\n        'location'  : B_DECR,\n        'diameter'  : B_DECR,\n        'direction'  : B_DECR,\n        'close'  : B_DECR,\n        'speed'  : B_DECR,\n        'hovered'  : B_DECR,\n        'between'  : B_DECR,\n        'object'  : B_DECR,\n        'glow'  : B_DECR,\n        'sister'  : B_DECR,\n        'clearly'  : B_DECR,\n        'never'  : B_DECR,\n        'trees'  : B_DECR,\n        'son'  : B_DECR,\n        'near'  : B_DECR,\n        'facing'  : B_DECR,\n        'movement'  : B_DECR,\n        'realized'  : B_DECR,\n        'photos'  : B_DECR,\n        'behind'  : B_DECR,\n        'witnessed'  : B_DECR,\n        'cloudy'  : B_DECR,\n        'immediate'  : B_DECR,\n        'notice'  : B_DECR,\n        'camera'  : B_DECR,\n        'large'  : B_DECR,\n        'brother'  : B_DECR,\n        'underneath'  : B_DECR,\n        'asked'  : B_DECR,\n        'closer'  : B_DECR,\n        'called'  : B_DECR,\n        'photo'  : B_DECR,\n        'yards'  : B_DECR,\n        'highway'  : B_DECR,\n        'down'  : B_DECR,\n        'objects'  : B_DECR,\n        'video'  : B_DECR,\n        'assumed'  : B_DECR,\n        'horizon'  : B_DECR,\n        'tree'  : B_DECR,\n        'station'  : B_DECR,\n        'mile'  : B_DECR,\n        'shape'  : B_DECR,\n        'together'  : B_DECR,\n        'ground'  : B_DECR,\n        'mountain'  : B_DECR,\n        'mother'  : B_DECR,\n        'observed'  : B_DECR,\n        'curious'  : B_DECR,\n        'fairly'  : B_DECR,\n        'feet'  : B_DECR,\n        'miles'  : B_DECR,\n        'ocean'  : B_DECR,\n        'below'  : B_DECR,\n        'brightness'  : B_DECR,\n        'bottom'  : B_DECR,\n        'hill'  : B_DECR,\n        'pattern'  : B_DECR,\n        'remember'  : B_DECR,\n        'daughter'  : B_DECR,\n        'hovering'  : B_DECR,\n        'joke'  : B_DECR,\n        'stationary'  : B_DECR,\n        'metallic'  : B_DECR,\n        'probably'  : B_DECR,\n        'top'  : B_DECR,\n        'might'  : B_DECR,\n        'approaching'  : B_DECR,\n        'slightly'  : B_DECR,\n        'smaller'  : B_DECR,\n        'toward'  : B_DECR,\n        'above'  : B_DECR,\n        'different'  : B_DECR,\n        'husband'  : B_DECR,\n        'glowing'  : B_DECR,\n        'mom'  : B_DECR,\n        'water'  : B_DECR,\n        'wasnt'  : B_DECR,\n        'path'  : B_DECR,\n        'friend'  : B_DECR,\n        'dad'  : B_DECR,\n        'observation'  : B_DECR,\n        'center'  : B_DECR,\n        'began'  : B_DECR,\n        'course'  : B_DECR,\n        'smoke'  : B_DECR,\n        'cigar'  : B_DECR,\n        'located'  : B_DECR,\n        'overhead'  : B_DECR,\n        'minutes'  : B_DECR,\n        'dog'  : B_DECR,\n        'witness'  : B_DECR,\n        'cold'  : B_DECR,\n        'report'  : B_DECR,\n        'wife'  : B_DECR,\n        'appears'  : B_DECR,\n        'bright'  : B_DECR,\n        'moon'  : B_DECR,\n        'moving'  : B_DECR,\n        'craft'  : B_DECR,\n        'brighter'  : B_DECR,\n    }\n\n    # check for special case idioms using a sentiment-laden keyword known to SAGE\n    SPECIAL_CASE_IDIOMS = {\n        \"the shit\": 3,\n        \"the bomb\": 3,\n        \"bad ass\": 1.5,\n        \"yeah right\": -2,\n        \"cut the mustard\": 2,\n        \"kiss of death\": -1.5,\n        \"hand to mouth\": -2,\n    }\n\n    # for removing punctuation\n    REGEX_REMOVE_PUNCTUATION = re.compile(f\"[{re.escape(string.punctuation)}]\")\n\n    PUNC_LIST = [\n        \".\",\n        \"!\",\n        \"?\",\n        \"&\",\n        \"&#44\",\n        \"&#39\",\n        \",\",\n        \";\",\n        \":\",\n        \"-\",\n        \"'\",\n        '\"',\n        \"!!\",\n        \"!!!\",\n        \"??\",\n        \"???\",\n        \"?!?\",\n        \"!?!\",\n        \"?!?!\",\n        \"!?!?\",\n    ]\n    \n    def __init__(self):\n        pass\n\n    \n    def negated(self, input_words, include_nt=True):\n        \"\"\"\n        Determine if input contains negation words\n        \"\"\"\n        neg_words = self.NEGATE\n        if any(word.lower() in neg_words for word in input_words):\n            return True\n        if include_nt:\n            if any(\"n't\" in word.lower() for word in input_words):\n                return True\n        #for first, second in pairwise(input_words):\n        #    if second.lower() == \"least\" and first.lower() != \"at\":\n        #        return True\n        return False\n\n\n    def normalize(self, score, alpha=15):\n        \"\"\"\n        Normalize the score to be between -1 and 1 using an alpha that\n        approximates the max expected value\n        \"\"\"\n        norm_score = score / math.sqrt((score * score) + alpha)\n        return norm_score\n\n\n    def scalar_inc_dec(self, word, valence, is_cap_diff):\n        \"\"\"\n        Check if the preceding words increase, decrease, or negate/nullify the\n        valence\n        \"\"\"\n        scalar = 0.0\n        word_lower = word.lower()\n        if word_lower in self.BOOSTER_DICT:\n            scalar = self.BOOSTER_DICT[word_lower]\n            if valence < 0:\n                scalar *= -1\n            # check if booster/dampener word is in ALLCAPS (while others aren't)\n            if word.isupper() and is_cap_diff:\n                if valence > 0:\n                    scalar += self.C_INCR\n                else:\n                    scalar -= self.C_INCR\n        return scalar\n\n\n\nclass SentiText:\n    \"\"\"\n    Identify sentiment-relevant string-level properties of input text.\n    \"\"\"\n\n    def __init__(self, text, punc_list, regex_remove_punctuation):\n        if not isinstance(text, str):\n            text = str(text.encode(\"utf-8\"))\n        self.text = text\n        self.PUNC_LIST = punc_list\n        self.REGEX_REMOVE_PUNCTUATION = regex_remove_punctuation\n        self.words_and_emoticons = self._words_and_emoticons()\n        # doesn't separate words from\n        # adjacent punctuation (keeps emoticons & contractions)\n        self.is_cap_diff = self.allcap_differential(self.words_and_emoticons)\n\n\n    def _words_plus_punc(self):\n        \"\"\"\n        Returns mapping of form:\n        {\n            'cat,': 'cat',\n            ',cat': 'cat',\n        }\n        \"\"\"\n        no_punc_text = self.REGEX_REMOVE_PUNCTUATION.sub(\"\", self.text)\n        # removes punctuation (but loses emoticons & contractions)\n        words_only = no_punc_text.split()\n        # remove singletons\n        words_only = {w for w in words_only if len(w) > 1}\n        # the product gives ('cat', ',') and (',', 'cat')\n        punc_before = {\"\".join(p): p[1] for p in product(self.PUNC_LIST, words_only)}\n        punc_after = {\"\".join(p): p[0] for p in product(words_only, self.PUNC_LIST)}\n        words_punc_dict = punc_before\n        words_punc_dict.update(punc_after)\n        return words_punc_dict\n\n    def _words_and_emoticons(self):\n        \"\"\"\n        Removes leading and trailing puncutation\n        Leaves contractions and most emoticons\n            Does not preserve punc-plus-letter emoticons (e.g. :D)\n        \"\"\"\n        wes = self.text.split()\n        words_punc_dict = self._words_plus_punc()\n        wes = [we for we in wes if len(we) > 1]\n        for i, we in enumerate(wes):\n            if we in words_punc_dict:\n                wes[i] = words_punc_dict[we]\n        return wes\n\n    def allcap_differential(self, words):\n        \"\"\"\n        Check whether just some words in the input are ALL CAPS\n\n        :param list words: The words to inspect\n        :returns: `True` if some but not all items in `words` are ALL CAPS\n        \"\"\"\n        is_different = False\n        allcap_words = 0\n        for word in words:\n            if word.isupper():\n                allcap_words += 1\n        cap_differential = len(words) - allcap_words\n        if 0 < cap_differential < len(words):\n            is_different = True\n        return is_different\n\n\n\nclass SentimentIntensityAnalyzer:\n    \"\"\"\n    Give a sentiment intensity score to sentences.\n    \"\"\"\n\n    def __init__(\n        self,\n        lexicon_file=\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\",\n    ):\n        self.lexicon_file = nltk.data.load(lexicon_file)\n        self.lexicon = self.make_lex_dict()\n        self.constants = VaderConstants()\n\n\n    def make_lex_dict(self):\n        \"\"\"\n        Convert lexicon file to a dictionary\n        \"\"\"\n        lex_dict = {}\n        for line in self.lexicon_file.split(\"\\n\"):\n            (word, measure) = line.strip().split(\"\\t\")[0:2]\n            lex_dict[word] = float(measure)\n        return lex_dict\n\n\n    def polarity_scores(self, text):\n        \"\"\"\n        Return a float for sentiment strength based on the input text.\n        Positive values are positive valence, negative value are negative\n        valence.\n\n        :note: Hashtags are not taken into consideration (e.g. #BAD is neutral). If you\n            are interested in processing the text in the hashtags too, then we recommend\n            preprocessing your data to remove the #, after which the hashtag text may be\n            matched as if it was a normal word in the sentence.\n        \"\"\"\n        # text, words_and_emoticons, is_cap_diff = self.preprocess(text)\n        sentitext = SentiText(\n            text, self.constants.PUNC_LIST, self.constants.REGEX_REMOVE_PUNCTUATION\n        )\n        sentiments = []\n        words_and_emoticons = sentitext.words_and_emoticons\n        for item in words_and_emoticons:\n            valence = 0\n            i = words_and_emoticons.index(item)\n            if (\n                i < len(words_and_emoticons) - 1\n                and item.lower() == \"kind\"\n                and words_and_emoticons[i + 1].lower() == \"of\"\n            ) or item.lower() in self.constants.BOOSTER_DICT:\n                sentiments.append(valence)\n                continue\n\n            sentiments = self.sentiment_valence(valence, sentitext, item, i, sentiments)\n\n        sentiments = self._but_check(words_and_emoticons, sentiments)\n\n        return self.score_valence(sentiments, text)\n\n\n    def sentiment_valence(self, valence, sentitext, item, i, sentiments):\n        is_cap_diff = sentitext.is_cap_diff\n        words_and_emoticons = sentitext.words_and_emoticons\n        item_lowercase = item.lower()\n        if item_lowercase in self.lexicon:\n            # get the sentiment valence\n            valence = self.lexicon[item_lowercase]\n\n            # check if sentiment laden word is in ALL CAPS (while others aren't)\n            if item.isupper() and is_cap_diff:\n                if valence > 0:\n                    valence += self.constants.C_INCR\n                else:\n                    valence -= self.constants.C_INCR\n\n            for start_i in range(0, 3):\n                if (\n                    i > start_i\n                    and words_and_emoticons[i - (start_i + 1)].lower()\n                    not in self.lexicon\n                ):\n                    # dampen the scalar modifier of preceding words and emoticons\n                    # (excluding the ones that immediately preceed the item) based\n                    # on their distance from the current item.\n                    s = self.constants.scalar_inc_dec(\n                        words_and_emoticons[i - (start_i + 1)], valence, is_cap_diff\n                    )\n                    if start_i == 1 and s != 0:\n                        s = s * 0.95\n                    if start_i == 2 and s != 0:\n                        s = s * 0.9\n                    valence = valence + s\n                    valence = self._never_check(\n                        valence, words_and_emoticons, start_i, i\n                    )\n                    if start_i == 2:\n                        valence = self._idioms_check(valence, words_and_emoticons, i)\n\n                        # future work: consider other sentiment-laden idioms\n                        # other_idioms =\n                        # {\"back handed\": -2, \"blow smoke\": -2, \"blowing smoke\": -2,\n                        #  \"upper hand\": 1, \"break a leg\": 2,\n                        #  \"cooking with gas\": 2, \"in the black\": 2, \"in the red\": -2,\n                        #  \"on the ball\": 2,\"under the weather\": -2}\n\n            valence = self._least_check(valence, words_and_emoticons, i)\n\n        sentiments.append(valence)\n        return sentiments\n\n\n    def _least_check(self, valence, words_and_emoticons, i):\n        # check for negation case using \"least\"\n        if (\n            i > 1\n            and words_and_emoticons[i - 1].lower() not in self.lexicon\n            and words_and_emoticons[i - 1].lower() == \"least\"\n        ):\n            if (\n                words_and_emoticons[i - 2].lower() != \"at\"\n                and words_and_emoticons[i - 2].lower() != \"very\"\n            ):\n                valence = valence * self.constants.N_SCALAR\n        elif (\n            i > 0\n            and words_and_emoticons[i - 1].lower() not in self.lexicon\n            and words_and_emoticons[i - 1].lower() == \"least\"\n        ):\n            valence = valence * self.constants.N_SCALAR\n        return valence\n\n    def _but_check(self, words_and_emoticons, sentiments):\n        words_and_emoticons = [w_e.lower() for w_e in words_and_emoticons]\n        but = {\"but\"} & set(words_and_emoticons)\n        if but:\n            bi = words_and_emoticons.index(next(iter(but)))\n            for sidx, sentiment in enumerate(sentiments):\n                if sidx < bi:\n                    sentiments[sidx] = sentiment * 0.5\n                elif sidx > bi:\n                    sentiments[sidx] = sentiment * 1.5\n        return sentiments\n\n    def _idioms_check(self, valence, words_and_emoticons, i):\n        onezero = f\"{words_and_emoticons[i - 1]} {words_and_emoticons[i]}\"\n\n        twoonezero = \"{} {} {}\".format(\n            words_and_emoticons[i - 2],\n            words_and_emoticons[i - 1],\n            words_and_emoticons[i],\n        )\n\n        twoone = f\"{words_and_emoticons[i - 2]} {words_and_emoticons[i - 1]}\"\n\n        threetwoone = \"{} {} {}\".format(\n            words_and_emoticons[i - 3],\n            words_and_emoticons[i - 2],\n            words_and_emoticons[i - 1],\n        )\n\n        threetwo = \"{} {}\".format(\n            words_and_emoticons[i - 3], words_and_emoticons[i - 2]\n        )\n\n        sequences = [onezero, twoonezero, twoone, threetwoone, threetwo]\n\n        for seq in sequences:\n            if seq in self.constants.SPECIAL_CASE_IDIOMS:\n                valence = self.constants.SPECIAL_CASE_IDIOMS[seq]\n                break\n\n        if len(words_and_emoticons) - 1 > i:\n            zeroone = f\"{words_and_emoticons[i]} {words_and_emoticons[i + 1]}\"\n            if zeroone in self.constants.SPECIAL_CASE_IDIOMS:\n                valence = self.constants.SPECIAL_CASE_IDIOMS[zeroone]\n        if len(words_and_emoticons) - 1 > i + 1:\n            zeroonetwo = \"{} {} {}\".format(\n                words_and_emoticons[i],\n                words_and_emoticons[i + 1],\n                words_and_emoticons[i + 2],\n            )\n            if zeroonetwo in self.constants.SPECIAL_CASE_IDIOMS:\n                valence = self.constants.SPECIAL_CASE_IDIOMS[zeroonetwo]\n\n        # check for booster/dampener bi-grams such as 'sort of' or 'kind of'\n        if (\n            threetwo in self.constants.BOOSTER_DICT\n            or twoone in self.constants.BOOSTER_DICT\n        ):\n            valence = valence + self.constants.B_DECR\n        return valence\n\n    def _never_check(self, valence, words_and_emoticons, start_i, i):\n        if start_i == 0:\n            if self.constants.negated([words_and_emoticons[i - 1]]):\n                valence = valence * self.constants.N_SCALAR\n        if start_i == 1:\n            if words_and_emoticons[i - 2] == \"never\" and (\n                words_and_emoticons[i - 1] == \"so\"\n                or words_and_emoticons[i - 1] == \"this\"\n            ):\n                valence = valence * 1.5\n            elif self.constants.negated([words_and_emoticons[i - (start_i + 1)]]):\n                valence = valence * self.constants.N_SCALAR\n        if start_i == 2:\n            if (\n                words_and_emoticons[i - 3] == \"never\"\n                and (\n                    words_and_emoticons[i - 2] == \"so\"\n                    or words_and_emoticons[i - 2] == \"this\"\n                )\n                or (\n                    words_and_emoticons[i - 1] == \"so\"\n                    or words_and_emoticons[i - 1] == \"this\"\n                )\n            ):\n                valence = valence * 1.25\n            elif self.constants.negated([words_and_emoticons[i - (start_i + 1)]]):\n                valence = valence * self.constants.N_SCALAR\n        return valence\n\n    def _punctuation_emphasis(self, sum_s, text):\n        # add emphasis from exclamation points and question marks\n        ep_amplifier = self._amplify_ep(text)\n        qm_amplifier = self._amplify_qm(text)\n        punct_emph_amplifier = ep_amplifier + qm_amplifier\n        return punct_emph_amplifier\n\n    def _amplify_ep(self, text):\n        # check for added emphasis resulting from exclamation points (up to 4 of them)\n        ep_count = text.count(\"!\")\n        if ep_count > 4:\n            ep_count = 4\n        # (empirically derived mean sentiment intensity rating increase for\n        # exclamation points)\n        ep_amplifier = ep_count * 0.292\n        return ep_amplifier\n\n    def _amplify_qm(self, text):\n        # check for added emphasis resulting from question marks (2 or 3+)\n        qm_count = text.count(\"?\")\n        qm_amplifier = 0\n        if qm_count > 1:\n            if qm_count <= 3:\n                # (empirically derived mean sentiment intensity rating increase for\n                # question marks)\n                qm_amplifier = qm_count * 0.18\n            else:\n                qm_amplifier = 0.96\n        return qm_amplifier\n\n    def _sift_sentiment_scores(self, sentiments):\n        # want separate positive versus negative sentiment scores\n        pos_sum = 0.0\n        neg_sum = 0.0\n        neu_count = 0\n        for sentiment_score in sentiments:\n            if sentiment_score > 0:\n                pos_sum += (\n                    float(sentiment_score) + 1\n                )  # compensates for neutral words that are counted as 1\n            if sentiment_score < 0:\n                neg_sum += (\n                    float(sentiment_score) - 1\n                )  # when used with math.fabs(), compensates for neutrals\n            if sentiment_score == 0:\n                neu_count += 1\n        return pos_sum, neg_sum, neu_count\n\n    def score_valence(self, sentiments, text):\n        if sentiments:\n            sum_s = float(sum(sentiments))\n            # compute and add emphasis from punctuation in text\n            punct_emph_amplifier = self._punctuation_emphasis(sum_s, text)\n            if sum_s > 0:\n                sum_s += punct_emph_amplifier\n            elif sum_s < 0:\n                sum_s -= punct_emph_amplifier\n\n            compound = self.constants.normalize(sum_s)\n            # discriminate between positive, negative and neutral sentiment scores\n            pos_sum, neg_sum, neu_count = self._sift_sentiment_scores(sentiments)\n\n            if pos_sum > math.fabs(neg_sum):\n                pos_sum += punct_emph_amplifier\n            elif pos_sum < math.fabs(neg_sum):\n                neg_sum -= punct_emph_amplifier\n\n            total = pos_sum + math.fabs(neg_sum) + neu_count\n            pos = math.fabs(pos_sum / total)\n            neg = math.fabs(neg_sum / total)\n            neu = math.fabs(neu_count / total)\n\n        else:\n            compound = 0.0\n            pos = 0.0\n            neg = 0.0\n            neu = 0.0\n\n        sentiment_dict = {\n            \"neg\": round(neg, 3),\n            \"neu\": round(neu, 3),\n            \"pos\": round(pos, 3),\n            \"compound\": round(compound, 4),\n        }\n\n        return sentiment_dict\n","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:04:14.65711Z","iopub.execute_input":"2023-09-26T02:04:14.658122Z","iopub.status.idle":"2023-09-26T02:04:14.767696Z","shell.execute_reply.started":"2023-09-26T02:04:14.658071Z","shell.execute_reply":"2023-09-26T02:04:14.766566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport pandas as pd\n!pip install xlrd\n# Pull in data\nmufon = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/Powell with Comments.xls',sheet_name='mufon_cms_2017-04-09',header=0)\nwitnesses = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/Powell with Comments.xls',sheet_name='Sheet1',header=0)\nmufon = mufon.dropna(how='all').copy()\nmufon.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:02:18.995567Z","iopub.execute_input":"2023-09-26T02:02:18.99661Z","iopub.status.idle":"2023-09-26T02:02:49.371056Z","shell.execute_reply.started":"2023-09-26T02:02:18.996556Z","shell.execute_reply":"2023-09-26T02:02:49.369733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# diminish the mufon dataset\nimport numpy as np\nmufon1 = mufon[mufon.Witnesses>0]\nmufon1 = mufon1[mufon1.Witnesses!=np.nan]\nmufon1 = mufon1[mufon1.Length<3000]","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:02:49.373542Z","iopub.execute_input":"2023-09-26T02:02:49.374495Z","iopub.status.idle":"2023-09-26T02:02:49.422393Z","shell.execute_reply.started":"2023-09-26T02:02:49.374459Z","shell.execute_reply":"2023-09-26T02:02:49.421061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#############################################\nlex = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/UFO lexicon rev2.xls',sheet_name='Sheet1',header=7)\nlex = lex.dropna(how='all').drop(columns='Unnamed: 0').copy()\n\n#lex[lex['RATING']!=0]\n#lex[lex['Previous Rating']!=0]\n# non-zero rating words\nlex_nonzero = lex[lex['RATING']!=0]\n\n#lex[lex['Previous Rating']>=3]\n# rating words gerater than or equal to 3\n#lex_nonzero = lex[lex['RATING']>=3]\nlex_nonzero\n#############################################","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:00:13.18127Z","iopub.execute_input":"2023-09-26T02:00:13.181632Z","iopub.status.idle":"2023-09-26T02:00:13.255047Z","shell.execute_reply.started":"2023-09-26T02:00:13.181601Z","shell.execute_reply":"2023-09-26T02:00:13.25385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nimport time\nfrom nltk.tokenize import word_tokenize\n\ndf_sample1 = mufon1.sample(1000)\n\n# hash through each comment to find only those that include non-zero lexicon words\nlexicon_favored1 = df_sample1\nlexicon_favored1['rating'] = pd.Series()\nlexicon_favored1['lexicon_word'] = pd.Series()\nlexicon_favored1['word_count'] = pd.Series()\n\nfor i,word in lex_nonzero.WORD.items():\n    for i2,piece in df_sample1['Detailed Description'].items():\n        if word in word_tokenize(piece.lower()):\n            #print('index',i2,'\\nword',word, '\\npiece',piece.lower(), '\\nrating', lex_nonzero.RATING[i],'\\n')\n\n            # add rating from lexicon\n            lexicon_favored1['rating'][i2] = lex_nonzero.RATING[i]\n\n            # add up every word usage in comments\n            lexicon_favored1['lexicon_word'][i2] = lex_nonzero.WORD[i]\n            \n            # word count\n            lexicon_favored1['word_count'][i2] = len(word_tokenize(piece.lower()))\n        else:\n            # word count\n            lexicon_favored1['word_count'][i2] = len(word_tokenize(piece.lower()))\n#clear_output()","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:14:31.09807Z","iopub.execute_input":"2023-09-26T02:14:31.0985Z","iopub.status.idle":"2023-09-26T02:51:43.335817Z","shell.execute_reply.started":"2023-09-26T02:14:31.098465Z","shell.execute_reply":"2023-09-26T02:51:43.334957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lexicon_favored1","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:51:43.337539Z","iopub.execute_input":"2023-09-26T02:51:43.337854Z","iopub.status.idle":"2023-09-26T02:51:43.374064Z","shell.execute_reply.started":"2023-09-26T02:51:43.337826Z","shell.execute_reply":"2023-09-26T02:51:43.372856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n# https://github.com/cjhutto/vaderSentiment\n#\n# Natural Language Toolkit: vader - TAMPERED w/ SCU Lexicon\ndef sentiment_scores(sentence):\n\n    # Create a SentimentIntensityAnalyzer object.\n    sid_obj = SentimentIntensityAnalyzer()\n    sentiment_dict = sid_obj.polarity_scores(sentence)\n    \n    # create a list\n    results = []\n    results.append({\"% Positive\":sentiment_dict['pos'],\n                    \"% Negative\":sentiment_dict['neg'],\n                    \"% Neutral\":sentiment_dict['neu'],\n                    \"% Compound\":sentiment_dict['compound']\n                   })\n    results = pd.DataFrame(results)\n    return results\n\n# Apply to df['comments'] column.\ndef NLP_PowellScore(commentsColumns):\n    \n    # obtain each comment for 'comments' column\n    eachComment = [eachComment for i,eachComment in commentsColumns.items()]\n    eachComment = pd.Series(eachComment)\n                               \n    # vader.variables.PowellScore\n    PowellPositive = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Positive\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellNegative = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Negative\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellNeutral = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Neutral\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellCompound = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Compound\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    \n    return PowellPositive,PowellNegative,PowellNeutral,PowellCompound","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:51:43.375578Z","iopub.execute_input":"2023-09-26T02:51:43.375894Z","iopub.status.idle":"2023-09-26T02:51:43.388445Z","shell.execute_reply.started":"2023-09-26T02:51:43.375867Z","shell.execute_reply":"2023-09-26T02:51:43.387228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#mufon = mufon.sample(30)\n# defining Powell Scores by sentiment outputs: Positive, Negative, & Neutral\nlexicon_favored1[\"PowellPositive\"] = NLP_PowellScore(lexicon_favored1['Detailed Description'])[0]\nlexicon_favored1[\"PowellNegative\"] = NLP_PowellScore(lexicon_favored1['Detailed Description'])[1]\nlexicon_favored1[\"PowellNeutral\"] = NLP_PowellScore(lexicon_favored1['Detailed Description'])[2]\nlexicon_favored1[\"PowellCompound\"] = NLP_PowellScore(lexicon_favored1['Detailed Description'])[3]\n\n# PowellScore \nlexicon_favored1[\"PowellScore\"] = (lexicon_favored1[\"PowellPositive\"]-lexicon_favored1[\"PowellNegative\"])/lexicon_favored1[\"PowellNeutral\"]\n\n# veracity\nlexicon_favored1[\"veracity\"] = lexicon_favored1[\"PowellScore\"]*lexicon_favored1[\"Length\"]*lexicon_favored1[\"Score\"]\n\ndf3 = lexicon_favored1.sort_values('veracity',ascending=False).reset_index(drop=True)\ndf3.head(100)\\\n        .style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:51:43.390591Z","iopub.execute_input":"2023-09-26T02:51:43.39096Z","iopub.status.idle":"2023-09-26T02:55:21.834335Z","shell.execute_reply.started":"2023-09-26T02:51:43.390928Z","shell.execute_reply":"2023-09-26T02:55:21.833285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lexicon_favored1.to_csv('lexicon_favored2.csv',index=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-26T02:55:21.835701Z","iopub.execute_input":"2023-09-26T02:55:21.836154Z","iopub.status.idle":"2023-09-26T02:55:21.940885Z","shell.execute_reply.started":"2023-09-26T02:55:21.836117Z","shell.execute_reply":"2023-09-26T02:55:21.939621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# en fin","metadata":{},"execution_count":null,"outputs":[]}]}