{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dascient/uacp-defining-powellscore-veracity-variables?scriptVersionId=144001201\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 2.0 UACP - Defining PowellScore & Veracity Variables\n## [1.0 UAP Analytic Centralization Program](https://www.kaggle.com/code/dascient/uacp-uap-analytic-centralization-program)\n<br>\n\n## [NLP - Sentiment Intensity Analyzer](https://github.com/cjhutto/vaderSentiment) Against Reporting Comments\n<br>\n\n### In collaboration with The Scientific Coalition for UAP Studies [(SCU)](ExploreSCU.org).\nHere we isolate only pertinent variables from the original dataset. We've also decided to leave open most of the code cells below; enabling transparency on foundation of all variables. ","metadata":{}},{"cell_type":"code","source":"%%time\n# for the sake of expeditious analysis\n!pip install xlrd\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import clear_output\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nfrom shapely.geometry import Point\nimport geopandas as gpd\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom geopandas import GeoDataFrame\nimport matplotlib.colors as colors\nimport seaborn as sns\nimport random as r\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        #print('Files loaded.')\n        \npd.set_option('display.max_colwidth', None)\n\n# loading first nuforc dataframe\nog_df1 = pd.read_csv('/kaggle/input/ufo-sightings/ufos.csv',header=0)\ndf = og_df1.dropna().copy()\nog_df2 = pd.read_csv('/kaggle/input/d/NUFORC/ufo-sightings/scrubbed.csv',header=0)\ndf2 = og_df2.dropna().copy()\n\n#############################################\n\nlex = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/UFO lexicon rev2.xls',sheet_name='Sheet1',header=7)\nlex = lex.dropna(how='all').drop(columns='Unnamed: 0').copy()\n\n#############################################\n# sanitize\n# drop some columns, for now\ndf = df.drop(columns=['datetime','duration (hours/min)'])\n\n# date posted deemed to be easily conveible to timestamp values, so i'm gonna work with that for now.\ndf['date posted'] = df['date posted'].astype('datetime64[ns]')\n\n\n# length of comments\ndf['comment_length'] = [len(str(v[0:500])) for i,v in df.comments.items()]\n\n\n# convert seconds to minutes\ndf[\"duration (minutes)\"] = [int(v)/60 for i,v in df[\"duration (seconds)\"].items()]\n\n\n# creating Geo Point column for sopecial use below\ndf['Geo Point'] = df.apply(lambda x:'%s, %s' % (x['latitude'],x['longitude']),axis=1)\n\n\n# let's create subsets of our 80,000 here: \n# we can implement conditionals, remove/analyze outliers, \n# & will enable for back referencing when starting to run \n# robust AI-ML modeling that would otherwise take much longer to run.\n\n# let's create subsets from the main dataframe/reporting-data w/ respect to duration of observations\ndf_under100 = df[df[\"duration (minutes)\"]<100]\ndf_under60 = df[df[\"duration (minutes)\"]<60]\n\n# random binary column for future AI-ML modeling.\na=['balloon','spacejunk','sensor_malfunction','undentified','anomalous']     \ndf['verified'] = pd.Series(r.choices(a,k=len(df),weights=(50, 40, 30, 20, 10)),index=df.index)\n\n# shape-focused\ncircles = df[df['shape'] == 'circle']\nspheres = df[df['shape'] == 'sphere']\nlights = df[df['shape'] == 'light']\nteardrops = df[df['shape'] == 'teardrop']\n\n# year-month\ndf['year_month'] = df['date posted'].dt.to_period('M')\n\nclear_output()\n# show\nprint(\"\\nOriginal dataset.\")\nprint(f\"\\nReports: {len(df)} non-null dataframe.\")\nprint(\"\\nMatrix:\",df.shape[0],\"rows,\",df.shape[1],\"columns\")\ndf = df.sort_values('date posted',ascending=True).reset_index(drop=True)\ndf.tail(11).reset_index(drop=True).style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'}).set_properties(**{'text-align': 'left'})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T06:59:26.570185Z","iopub.execute_input":"2023-09-23T06:59:26.571077Z","iopub.status.idle":"2023-09-23T06:59:48.078794Z","shell.execute_reply.started":"2023-09-23T06:59:26.571017Z","shell.execute_reply":"2023-09-23T06:59:48.077133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lexicon","metadata":{}},{"cell_type":"code","source":"#lex[lex['RATING']!=0]\n#lex[lex['Previous Rating']!=0]\n# non-zero rating words\nlex_nonzero = lex[lex['RATING']!=0]\n\n#lex[lex['Previous Rating']>=3]\n# rating words gerater than or equal to 3\n#lex_nonzero = lex[lex['RATING']>=3]\nlex_nonzero","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T06:59:48.081576Z","iopub.execute_input":"2023-09-23T06:59:48.08197Z","iopub.status.idle":"2023-09-23T06:59:48.120072Z","shell.execute_reply.started":"2023-09-23T06:59:48.081939Z","shell.execute_reply":"2023-09-23T06:59:48.118654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hash through each comment to find only those that include non-zero lexicon words. ","metadata":{}},{"cell_type":"code","source":"%%time\nimport time\nfrom nltk.tokenize import word_tokenize\n\ndf_sample = df.sample(1000)\n\n# hash through each comment to find only those that include non-zero lexicon words\nlexicon_favored = df_sample.copy()\nlexicon_favored['rating'] = pd.Series()\nlexicon_favored['lexicon_word'] = pd.Series()\nlexicon_favored['word_count'] = pd.Series()\n\nfor i,word in lex_nonzero.WORD.items():\n    for i2,piece in df_sample.comments.items():     \n        if word in word_tokenize(piece.lower()):\n            #print('index',i2,'\\nword',word, '\\npiece',piece.lower(), '\\nrating', lex_nonzero.RATING[i],'\\n')\n\n            # add rating from lexicon\n            lexicon_favored['rating'][i2] = lex_nonzero.RATING[i]\n\n            # add up every word usage in comments\n            lexicon_favored['lexicon_word'][i2] = lex_nonzero.WORD[i]\n            \n            # word count\n            lexicon_favored['word_count'][i2] = len(word_tokenize(piece.lower()))\n        else:\n            # word count\n            lexicon_favored['word_count'][i2] = len(word_tokenize(piece.lower()))\n            \nlexicon_favored = lexicon_favored\n#clear_output()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-23T06:59:48.121989Z","iopub.execute_input":"2023-09-23T06:59:48.122378Z","iopub.status.idle":"2023-09-23T07:04:06.859677Z","shell.execute_reply.started":"2023-09-23T06:59:48.122345Z","shell.execute_reply":"2023-09-23T07:04:06.858086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lexicon-Focused Dataset\n<br>\n\n#### Snippet","metadata":{}},{"cell_type":"code","source":"lexicon_favored.head()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T07:04:06.861573Z","iopub.execute_input":"2023-09-23T07:04:06.862569Z","iopub.status.idle":"2023-09-23T07:04:06.896122Z","shell.execute_reply.started":"2023-09-23T07:04:06.862525Z","shell.execute_reply":"2023-09-23T07:04:06.893889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"any(lexicon_favored.index.duplicated())","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-23T07:04:06.899735Z","iopub.execute_input":"2023-09-23T07:04:06.900996Z","iopub.status.idle":"2023-09-23T07:04:06.915732Z","shell.execute_reply.started":"2023-09-23T07:04:06.90094Z","shell.execute_reply":"2023-09-23T07:04:06.914477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n_ = plt.hist(lexicon_favored.word_count, bins='auto')  # arguments are passed to np.histogram\nplt.title(\"Word Counts Histogram with 'auto' bins\")\nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T07:04:06.91766Z","iopub.execute_input":"2023-09-23T07:04:06.919755Z","iopub.status.idle":"2023-09-23T07:04:07.305785Z","shell.execute_reply.started":"2023-09-23T07:04:06.919704Z","shell.execute_reply":"2023-09-23T07:04:07.304362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stopwords\nimport nltk\nfrom nltk.corpus import stopwords\n \n#nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\nlexicon_favored['just_words'] = pd.Series()\nfiltered_sentence = []\n\nfor i,piece in lexicon_favored.comments.items():     \n    for word in word_tokenize(piece.lower()):\n        if word not in stop_words:\n            filtered_sentence.append({i, word})","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T07:04:07.307453Z","iopub.execute_input":"2023-09-23T07:04:07.307851Z","iopub.status.idle":"2023-09-23T07:04:07.61415Z","shell.execute_reply.started":"2023-09-23T07:04:07.307819Z","shell.execute_reply":"2023-09-23T07:04:07.612709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ranked by Lexicon Rating.","metadata":{}},{"cell_type":"code","source":"lexicon_favored = lexicon_favored.sort_values('rating',ascending=False).reset_index(drop=True)\nlexicon_favored.head(25).style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2023-09-23T07:04:07.616015Z","iopub.execute_input":"2023-09-23T07:04:07.617763Z","iopub.status.idle":"2023-09-23T07:04:07.687688Z","shell.execute_reply.started":"2023-09-23T07:04:07.617714Z","shell.execute_reply":"2023-09-23T07:04:07.686315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# \"Be careful not to choke on your aspirations.\" - Darth Vader\n### Application of [VADER](https://github.com/cjhutto/vaderSentiment) (Valence Aware Dictionary and sEntiment Reasoner)\nA lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media.","metadata":{}},{"cell_type":"code","source":"# https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n# https://github.com/cjhutto/vaderSentiment\n# import SentimentIntensityAnalyzer class\n# from vaderSentiment.vaderSentiment module.\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# function to print sentiments\n# of the sentence.\ndef sentiment_scores(sentence):\n\n    # Create a SentimentIntensityAnalyzer object.\n    sid_obj = SentimentIntensityAnalyzer()\n    sentiment_dict = sid_obj.polarity_scores(sentence)\n    \n    # create a list\n    results = []\n    results.append({\"% Positive\":sentiment_dict['pos'],\n                    \"% Negative\":sentiment_dict['neg'],\n                    \"% Neutral\":sentiment_dict['neu']\n                   })\n    results = pd.DataFrame(results)\n    return results\n\n# Apply to df['comments'] column.\ndef NLP_PowellScore(commentsColumns):\n    \n    # obtain each comment for 'comments' column\n    eachComment = [eachComment for i,eachComment in commentsColumns.items()]\n    eachComment = pd.Series(eachComment)\n                               \n    # vader.variables.PowellScore\n    PowellPositive = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Positive\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellNegative = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Negative\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    PowellNeutral = [v for v in list([sentiment_scores(sentimentAnalyzedComment)[\"% Neutral\"][0] for i,sentimentAnalyzedComment in eachComment.items()])]\n    \n    return PowellPositive,PowellNegative,PowellNeutral","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:04:07.6893Z","iopub.execute_input":"2023-09-23T07:04:07.690546Z","iopub.status.idle":"2023-09-23T07:04:07.715997Z","shell.execute_reply.started":"2023-09-23T07:04:07.690499Z","shell.execute_reply":"2023-09-23T07:04:07.71495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Reports sorted by Veracity\nWe also added the \"Rating\" score from Lexicon.","metadata":{}},{"cell_type":"code","source":"# let's only take a small sample - this will definitely take a few minutes, grab yourself some water...\nrobert = lexicon_favored.copy()#.sample(30000)\nrobert.rating.value_counts()","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-23T07:04:07.717938Z","iopub.execute_input":"2023-09-23T07:04:07.719755Z","iopub.status.idle":"2023-09-23T07:04:07.731373Z","shell.execute_reply.started":"2023-09-23T07:04:07.719705Z","shell.execute_reply":"2023-09-23T07:04:07.730005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# defining Powell Scores by sentiment outputs: Positive, Negative, Neutral, & Rating\nrobert[\"PowellPositive\"] = NLP_PowellScore(robert['comments'])[0]\nrobert[\"PowellNegative\"] = NLP_PowellScore(robert['comments'])[1]\nrobert[\"PowellNeutral\"] = NLP_PowellScore(robert['comments'])[2]\n\n# PowellScore \nrobert[\"PowellScore\"] = (robert[\"PowellPositive\"]-robert[\"PowellNegative\"])/robert[\"PowellNeutral\"]\n\n# veracity\nrobert[\"veracity\"] = robert[\"PowellScore\"]*robert[\"comment_length\"]*robert[\"rating\"] # FINALLY, THIS EQUATION ACCOUNTS FOR POWELL'S LEXICON RATINGS!\n\n# veracity is still very much in progress. we are looking for ways forward to \n# better define them. although, it is important to note that \"veracity\" will \n# be variable that is subjective to the type of datasets.\ncolumns = ['date posted','city','state','shape','comments',\n           'comment_length','latitude','longitude','duration (minutes)',\\\n           'PowellScore','veracity','rating','lexicon_word']\n\ndf1 = robert[columns].sort_values('veracity',ascending=False).reset_index(drop=True)\ndf1[df1['comment_length']>10].head(20)\\\n        .style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:04:07.733276Z","iopub.execute_input":"2023-09-23T07:04:07.733643Z","iopub.status.idle":"2023-09-23T07:05:49.940914Z","shell.execute_reply.started":"2023-09-23T07:04:07.733614Z","shell.execute_reply":"2023-09-23T07:05:49.939132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Ovals seen in California","metadata":{}},{"cell_type":"code","source":"%%time\n# ca_oval\nca_oval = df1[df1.state=='ca'].reset_index(drop=True)\nca_oval = ca_oval[ca_oval['shape']=='oval']\n\n# only ovals\nca_oval_162 = ca_oval.sort_values(['latitude','longitude'])\nrobert_ca_oval_162 = ca_oval_162\nrobert_ca_oval_162[\"PowellPositive\"] = NLP_PowellScore(robert_ca_oval_162['comments'])[0]\nrobert_ca_oval_162[\"PowellNegative\"] = NLP_PowellScore(robert_ca_oval_162['comments'])[1]\nrobert_ca_oval_162[\"PowellNeutral\"] = NLP_PowellScore(robert_ca_oval_162['comments'])[2]\n\n# PowellScore \nrobert_ca_oval_162[\"PowellScore\"] = (robert_ca_oval_162[\"PowellPositive\"]-robert_ca_oval_162[\"PowellNegative\"])/robert_ca_oval_162[\"PowellNeutral\"]\n\n# veracity\nrobert_ca_oval_162[\"veracity\"] = robert_ca_oval_162[\"PowellScore\"]*robert_ca_oval_162[\"comment_length\"]*robert_ca_oval_162[\"rating\"]\n\n\ndf2 = robert_ca_oval_162[columns].sort_values(['veracity'],ascending=False).reset_index(drop=True)\ndf2.head(50).style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:49.942404Z","iopub.execute_input":"2023-09-23T07:05:49.942901Z","iopub.status.idle":"2023-09-23T07:05:50.30859Z","shell.execute_reply.started":"2023-09-23T07:05:49.942858Z","shell.execute_reply":"2023-09-23T07:05:50.307304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Powell Variables in 3D\nThis is a 3D-interactive chart that uses the date posted, veracity, & PowellScore variables. Colored by lexicon rating. Sized by comment_length.\n\nBy definition, these actually render 5-Dimensional charts, if one considers veracity & commenth lengths of reports as 'features of a situation'.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\ndf_plot = df1[df1.rating>0]\n# graph\nfig = px.scatter_3d(df_plot, x='date posted', y='veracity', z='PowellScore',\n              color='rating',\n              size = 'comment_length',\n              hover_name = 'city',\n              hover_data=['city','state','comments','rating','lexicon_word'],              \n              opacity=0.5,\n              size_max=17\n                   )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:50.310107Z","iopub.execute_input":"2023-09-23T07:05:50.310627Z","iopub.status.idle":"2023-09-23T07:05:53.660128Z","shell.execute_reply.started":"2023-09-23T07:05:50.310584Z","shell.execute_reply":"2023-09-23T07:05:53.658711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### This one shows Date Posted vs PowellScore & Lexicon Rating Variables of California Oval reports. Colored by Veracity.","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# graph\nfig = px.scatter_3d(df1, x='date posted', y='PowellScore', z='rating',\n              color='veracity',\n              size = 'comment_length',\n              hover_name = 'city',\n              hover_data=['city','state','comments','rating','lexicon_word'],              \n              opacity=0.5,\n              size_max=17\n                   )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:53.666913Z","iopub.execute_input":"2023-09-23T07:05:53.668336Z","iopub.status.idle":"2023-09-23T07:05:53.865587Z","shell.execute_reply.started":"2023-09-23T07:05:53.668283Z","shell.execute_reply":"2023-09-23T07:05:53.864508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Ovals seen in California - Date Posted vs PowellNeutral vs PowellScore","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\n# graph\nfig = px.scatter_3d(robert_ca_oval_162, x='date posted', y='PowellNeutral', z='rating',\n              color='veracity',\n              size = 'comment_length',\n              hover_name = 'city',\n              hover_data=['city','state','comments','shape','veracity','rating'],              \n              opacity=0.5,\n              size_max=17\n                   )\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:53.867187Z","iopub.execute_input":"2023-09-23T07:05:53.867961Z","iopub.status.idle":"2023-09-23T07:05:53.973106Z","shell.execute_reply.started":"2023-09-23T07:05:53.867916Z","shell.execute_reply":"2023-09-23T07:05:53.971579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These variables are still very much in progress & there currently is no process for defining them. Despite the disparate, disconnected, & wide range of skeptic/non-skeptic relational databases — we have managed to connect with organizations that promote open source — public repositories & most are willing to coordinate with one another in developing a UAP Reporting & Events Hub. Wherein all pertinent reports, sightings, measurements, & signatures are to be populated by various factors from multiple disciplines & technologies. We will do our best to coordinate with prominent key members of the UAP community in order to contribute to building out a “standardized” reporting mechanism in an intelligible & non-duplicative fashion. We are looking for ways forward in getting access to real-time, current reports.\n\nThe goal would be to create something similar to an Order of Battle, so that reports at specific times & locations can be compared to past reports to augment credibility determination, as well as eventually be compared to known events that may explain them. Once those explanations are vetted, reports would be coded by likelihood of mundane vs anomalous, which would aid in the processing of similar events in the future.\n\nIn addition, we have already begun looking for trends over time, such as the time of day when reported events take place, & the type of object reported over the decades. The latter can be observed in the “Shapes by Share of Reports” chart, which provides indications of confirmation bias in observed behavior.\n\nFinally, big data analysis (alongside robust AI|ML|DS modeling techniques) could also provide insight into the development of improved collection & reporting processes, which currently appear to be undefined, improving the quality of the data we receive. — K. Kolbe.","metadata":{}},{"cell_type":"markdown","source":"# Different NLP Methods","metadata":{}},{"cell_type":"markdown","source":"# DaS-VADER Sentiment Analyzer\n\nHere, we begin focusing on the lexicon analysis of each comment submission.","metadata":{}},{"cell_type":"code","source":"df1[['comments']].sample(15).style.set_properties(**{'text-align': 'left'})","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:53.974843Z","iopub.execute_input":"2023-09-23T07:05:53.975236Z","iopub.status.idle":"2023-09-23T07:05:53.99043Z","shell.execute_reply.started":"2023-09-23T07:05:53.975205Z","shell.execute_reply":"2023-09-23T07:05:53.989225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets split up every word from every column & frame that in itself, call it \"words\"\na = [v.split(' ') for i,v in df1.comments.items()]\nflatlist=[]\nfor sublist in a:\n    for element in sublist:\n        flatlist.append(element)\ncomments = pd.DataFrame(flatlist, columns=['words'])\ncomments","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:53.992325Z","iopub.execute_input":"2023-09-23T07:05:53.992824Z","iopub.status.idle":"2023-09-23T07:05:54.02242Z","shell.execute_reply.started":"2023-09-23T07:05:53.992781Z","shell.execute_reply":"2023-09-23T07:05:54.021184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\n\ncomments.words.value_counts().head(20)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:54.023813Z","iopub.execute_input":"2023-09-23T07:05:54.024254Z","iopub.status.idle":"2023-09-23T07:05:54.04808Z","shell.execute_reply.started":"2023-09-23T07:05:54.024211Z","shell.execute_reply":"2023-09-23T07:05:54.046433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comments.words.value_counts().head(50).plot(kind='barh',figsize=(15,7))","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:54.049617Z","iopub.execute_input":"2023-09-23T07:05:54.050012Z","iopub.status.idle":"2023-09-23T07:05:54.815066Z","shell.execute_reply.started":"2023-09-23T07:05:54.049979Z","shell.execute_reply":"2023-09-23T07:05:54.813368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# billy-boy!\n# isolate only \"adjectives, nouns, verbs, & adverbs\"","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:54.816826Z","iopub.execute_input":"2023-09-23T07:05:54.817949Z","iopub.status.idle":"2023-09-23T07:05:54.822937Z","shell.execute_reply.started":"2023-09-23T07:05:54.817912Z","shell.execute_reply":"2023-09-23T07:05:54.821697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visual Insights","metadata":{}},{"cell_type":"markdown","source":"## Word Clouds\n### All 500 samples.","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# in the clouds\n# 500 samples \ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df1.comments:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 7,\n                colormap='twilight').generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize=(25,10), facecolor='None')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T07:05:54.824644Z","iopub.execute_input":"2023-09-23T07:05:54.825116Z","iopub.status.idle":"2023-09-23T07:05:57.162571Z","shell.execute_reply.started":"2023-09-23T07:05:54.825085Z","shell.execute_reply":"2023-09-23T07:05:57.161204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### California Ovals ","metadata":{}},{"cell_type":"code","source":"# in the clouds\n# california ovals\ncomment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df2.comments:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 1200, height = 600,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 7,\n                colormap='twilight').generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize=(25,10), facecolor='None')\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-23T07:05:57.164105Z","iopub.execute_input":"2023-09-23T07:05:57.16454Z","iopub.status.idle":"2023-09-23T07:05:58.341518Z","shell.execute_reply.started":"2023-09-23T07:05:57.164505Z","shell.execute_reply":"2023-09-23T07:05:58.340469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Top Veracity Comments","metadata":{}},{"cell_type":"code","source":"df1.reset_index(drop=True).set_index('veracity').head(20)[['comments','rating','lexicon_word','shape','city']]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:58.342798Z","iopub.execute_input":"2023-09-23T07:05:58.343474Z","iopub.status.idle":"2023-09-23T07:05:58.367993Z","shell.execute_reply.started":"2023-09-23T07:05:58.343437Z","shell.execute_reply":"2023-09-23T07:05:58.366668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### California Ovals Comments","metadata":{}},{"cell_type":"code","source":"df2.set_index('veracity')[['comments','shape','city']]","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:05:58.37082Z","iopub.execute_input":"2023-09-23T07:05:58.371336Z","iopub.status.idle":"2023-09-23T07:05:58.391524Z","shell.execute_reply.started":"2023-09-23T07:05:58.371301Z","shell.execute_reply":"2023-09-23T07:05:58.39027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MUFON Data Exploitation","metadata":{}},{"cell_type":"code","source":"%%time\n# Pull in data\nmufon = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/Powell with Comments.xls',sheet_name='mufon_cms_2017-04-09',header=0)\nwitnesses = pd.read_excel('/kaggle/input/scu-nlp-uap-lexicon/Powell with Comments.xls',sheet_name='Sheet1',header=0)\nmufon = mufon.dropna(how='all').copy()\nmufon.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#mufon = mufon.sample(30)\n# defining Powell Scores by sentiment outputs: Positive, Negative, & Neutral\nmufon[\"PowellPositive\"] = NLP_PowellScore(mufon['Detailed Description'])[0]\nmufon[\"PowellNegative\"] = NLP_PowellScore(mufon['Detailed Description'])[1]\nmufon[\"PowellNeutral\"] = NLP_PowellScore(mufon['Detailed Description'])[2]\n\n# PowellScore \nmufon[\"PowellScore\"] = (mufon[\"PowellPositive\"]-mufon[\"PowellNegative\"])/mufon[\"PowellNeutral\"]\n\n# veracity\nmufon[\"veracity\"] = mufon[\"PowellScore\"]*mufon[\"Length\"]*mufon[\"Score\"]\n\ndf3 = mufon.sort_values('veracity',ascending=False).reset_index(drop=True)\ndf3.head(100)\\\n        .style.background_gradient(cmap ='seismic').set_properties(**{'font-size': '11px'})","metadata":{"execution":{"iopub.status.busy":"2023-09-23T18:11:36.550319Z","iopub.execute_input":"2023-09-23T18:11:36.551441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mufon.to_csv(index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T18:07:40.302191Z","iopub.execute_input":"2023-09-23T18:07:40.30266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Science","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# encoding\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode(df):\n    lb_make = LabelEncoder()\n    columns = df.columns.values.tolist()\n    df_encoded = df[columns].copy()\n\n    # categorize/encode\n    for i in columns:\n        df_encoded[i] = lb_make.fit_transform(df[i])\n\n    # encoded\n    return df_encoded\n\n\n# encoded variable re-mapping\ndef encoding_remap(df, df_encoded, target):\n    \n    X_test = X_y_sets(df, target)[0][0]\n    \n    remap = pd.merge(df_encoded.loc[df_encoded.index.isin(X_test.index.values)][target].reset_index(),\n              df.loc[df.index.isin(X_test.index.values)][[target]].reset_index(),on=['index'])\n    \n    remap[target] = [str(remap[f'{target}_y'][i]) for i,v in remap[f'{target}_x'].items()]\n    remap['index'] = np.array([str(remap[f'{target}_x'][i]) for i,v in remap[f'{target}_x'].items()]).astype(int)\n    remap=remap[[target,'index']]\n    remap = remap.set_index('index').drop_duplicates().sort_values('index')\n    \n    return remap\n\n\n# pairplot\nimport seaborn as sns\ndef pairplot(df, target):\n    return sns.pairplot(df.sample(int(len(df/10000))),hue=target)\n    \n    \n# create X,y variables for ML\nfrom sklearn.model_selection import train_test_split\ndef X_y_sets(df, target):\n    X = df.dropna().drop(columns=[target]).copy()\n    y = df.dropna()[target].ravel().copy()\n    \n    return train_test_split(X, y, test_size=0.33, random_state=42), X, y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# is scaling necessary?\n# construction of ML dataframes\ntarget = 'Disposition'#''veracity','PowellScore','NLP_PowellScore'\n\n# copy\na = df[['Witnesses','Score','Length','Nearest City','State',\\\n        'Object Shape Primary','Exact Latitude','Exact Longitude','Disposition','veracity','PowellScore']].copy()\n\n# for the sake of computationa efficiency\na = a.head(10000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find random sample & save index for defining an encoded use-case\nfrom random import randrange\nidx = randrange(len(a))\n\n# print random configuration item\nprint(\"\\nThis is a randomly chosen subject we will try to predict.\")\nb = pd.DataFrame(a.loc[idx]).T\nprint(f\"\\nTarget:'{target}' value is: \",b.reset_index()[target][0],\"\\n\")\n\n# store sol'n\nsolution = str(b.reset_index()[target][0])\n\n# print data point\nb\n# if this cell fails, try it again from step 1 - you ran into a null variable (i'll fix that soon enough)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorize/encode entire dataframe(a)\nc = encode(a)\nprint(\"\\nOriginal dataframe encoded into something we can run a classifier against.\\n\")\nc.sample(10).reset_index(drop=True).style.background_gradient(cmap ='Pastel1').set_properties(**{'font-size': '10px'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 'comments' & 'country' - out\nsns.pairplot(c[['city','state','latitude','longitude','year_month','comment_length','verified','veracity','PowellScore']]\\\n             .sample(100).copy(),\n             hue=f'{target}',\n             kind=\"kde\",\n             corner=True,\n             palette=\"Paired\"\n            )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print encoded item\nuse_case = pd.DataFrame(c.loc[idx]).T.drop(columns=[target]) \n\n#c\n\n# print encoded item w/out target info\ndata = c.drop(columns=[target]) \n\nprint(\"\\nThis is what our encoded 'use-case' looks like - number form, just the way the machine likes it.\\n\")\n\nuse_case.style.background_gradient(cmap ='twilight').set_properties(**{'font-size': '10px'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create X,y variables for ML\n# save trainer\nprint(\"\\nResetting train data...\\nCreating X-matrix & y-vector (target) for classification.\")\ntrainer = c.loc[c.index!=idx].copy()\nX, y =  trainer.drop(columns=[target]), trainer[target].ravel()\nX_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['target'] = pd.Series(y_train)\nX_train.dropna().head().reset_index(drop=True).reset_index(drop=True).style.background_gradient(cmap ='twilight').set_properties(**{'font-size': '10px'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for the sake of adding the 'target' column above for sake of layman's explanation\nX_train, X_test, y_train, y_test = X_y_sets(trainer, target)[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encoded variable re-mapping\n# specific to our current target choice\nd = encoding_remap(a, c, target)\nprint(\"\\nDecoding our encoded dataframe to correlate with the initial randomly chosen subject.\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"\\n-Live prediction-\\nThinking...\\n\")\n\n# MLP\nclf = MLPClassifier(alpha=0.666, max_iter=666).fit(X_train, y_train)\n\nprint()\nprint(\"Test score (confidence): \",clf.score(X_test, y_test)*100,\"%\")\nprint()\nprediction = clf.predict(use_case)[0]\nprint(f\"Prediction {target} index:\",prediction)\n\n# print decoded prediction\nprint(\"\\nPrediction Decoded\")\ne = d[d.index == prediction]\ne\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"solved = str(e.verified[e.index[0]])\nif solution == solved:\n    print(f\"\\nYUP!\\n\\nThe machine's prediction against target variable '{target}' was correct!\\n\")\nelse:\n    print(\"\\nNOPE!\\nThe machine's prediction was incorrect :(\")\n    \nprint()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# en fin","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}